---
title: "Task-Agnostic Amortized Inference of Gaussian Process Hyperparameters"
authors:
- admin
- Xingyuan Sun
- Peter J. Ramadge
- Ryan P. Adams
date: "2020-07-18T00:00:00Z"
doi: ""

# Schedule page publish date (NOT publication's date).
#publishDate: "2017-01-01T00:00:00Z"

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ["3"]

# Publication name and optional abbreviated publication name.
publication: Advances in Neural Information Processing Systems (NeurIPS). Prelimary version at ICML Workshop on Automated Machine Learning

abstract: Gaussian processes (GPs) are flexible priors for modeling functions. However, their success depends on the kernel accurately reflecting the properties of the data. One of the appeals of the GP framework is that the marginal likelihood of the kernel hyperparameters is often available in closed form, enabling optimization and sampling procedures to fit these hyperparameters to data. Unfortunately, point-wise evaluation of the marginal likelihood is expensive due to the need to solve a linear system; searching or sampling the space of hyperparameters thus often dominates the practical cost of using GPs. We introduce an approach to the identification of kernel hyperparameters in GP regression and related problems that sidesteps the need for costly marginal likelihoods. Our strategy is to “amortize” inference over hyperparameters by training a single neural network, which consumes a set of regression data and produces an estimate of the kernel function, useful across different tasks. To accommodate the varying dimension and cardinality of different regression problems, we use a hierarchical self-attention-based neural network that produces estimates of the hyperparameters which are invariant to the order of the input data points and data dimensions. We show that a single neural model trained on synthetic data is able to generalize directly to several different real-world GP use cases. Our experiments demonstrate that the estimated hyperparameters are comparable in quality to those from the conventional model selection procedures, while being much faster to obtain, significantly accelerating GP regression and its related applications such as Bayesian optimization and Bayesian quadrature.

# Summary. An optional shortened abstract.
summary:

tags:
- AutoML
- Gaussian process
- meta-learning
featured: true

links:
- name: Preprint at AutoML workshop
  url: https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_54.pdf
- name: Code coming soon
  url: 
#url_pdf: https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_54.pdf
#url_code: '#'
#url_dataset: '#'
#url_poster: https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_54_poster.pdf
#url_project: ''
#url_slides: ''
#url_source: '#'
url_video: https://slideslive.com/38930662/taskagnostic-amortized-inference-of-gaussian-process-hyperparameters

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
image:
  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/pLCdAaMFLTE)'
  focal_point: ""
  preview_only: true

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects:
- 

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: ""
---
